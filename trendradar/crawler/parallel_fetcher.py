# coding=utf-8
"""
å¹¶è¡Œæ•°æ®è·å–å™¨ - ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæå‡æ•°æ®æŠ“å–é€Ÿåº¦
"""

import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Optional, Union
import requests

from trendradar.crawler.fetcher import DataFetcher


class ParallelDataFetcher:
    """å¹¶è¡Œæ•°æ®è·å–å™¨ - ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–å¤šä¸ªå¹³å°æ•°æ®"""

    def __init__(
        self,
        proxy_url: Optional[str] = None,
        api_url: Optional[str] = None,
        max_workers: int = 5,
        timeout: int = 15,
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œæ•°æ®è·å–å™¨

        Args:
            proxy_url: ä»£ç†æœåŠ¡å™¨ URL
            api_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.proxy_url = proxy_url
        self.api_url = api_url or DataFetcher.DEFAULT_API_URL
        self.max_workers = max_workers
        self.timeout = timeout

        # ä»£ç†é…ç½®
        self.proxies = None
        if self.proxy_url:
            self.proxies = {"http": self.proxy_url, "https": self.proxy_url}

    def _fetch_single_data(
        self, id_info: Union[str, Tuple[str, str]]
    ) -> Tuple[Optional[str], str, str]:
        """
        è·å–å•ä¸ªå¹³å°çš„æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        Args:
            id_info: å¹³å°IDæˆ– (å¹³å°ID, åˆ«å)

        Returns:
            (å“åº”æ–‡æœ¬, å¹³å°ID, åˆ«å)
        """
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"{self.api_url}?id={id_value}"

        proxies = self.proxies

        try:
            # ä½¿ç”¨ Session ä¿æŒè¿æ¥å¤ç”¨
            with requests.Session() as session:
                session.headers.update(DataFetcher.DEFAULT_HEADERS)

                start_time = time.time()
                response = session.get(url, proxies=proxies, timeout=self.timeout)

                elapsed = time.time() - start_time

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")

                if status not in ["success", "cache"]:
                    return None, id_value, alias

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"

                if elapsed > 0.5:
                    print(
                        f"âœ… {id_value} è·å–æˆåŠŸ ({status_info}) - è€—æ—¶ {elapsed:.2f}ç§’"
                    )
                else:
                    print(
                        f"âœ“ {id_value} è·å–æˆåŠŸ ({status_info}) - è€—æ—¶ {elapsed:.2f}ç§’"
                    )

                return data_text, id_value, alias

        except requests.Timeout:
            print(f"â± {id_value} è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰")
            return None, id_value, alias
        except Exception as e:
            print(f"âŒ {id_value} è¯·æ±‚å¤±è´¥: {e}")
            return None, id_value, alias

    def fetch_all_parallel(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        progress_callback: Optional[callable] = None,
        request_interval: int = 100,
    ) -> Tuple[Dict, Dict, List]:
        """
        å¹¶è¡Œè·å–æ‰€æœ‰å¹³å°æ•°æ®

        Args:
            ids_list: å¹³å°IDåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–(å¹³å°ID, åˆ«å)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°(progress, total, current)
            request_interval: è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰ï¼Œä»…ç”¨äºæ—¥å¿—æ˜¾ç¤º

        Returns:
            (ç»“æœå­—å…¸, IDåˆ°åç§°æ˜ å°„, å¤±è´¥IDåˆ—è¡¨)
        """
        if not ids_list:
            print("âš ï¸ æ²¡æœ‰éœ€è¦è·å–çš„å¹³å°ID")
            return {}, {}, []

        results = {}
        id_to_name = {}
        failed_ids = []
        total = len(ids_list)

        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œè·å– {total} ä¸ªå¹³å°æ•°æ®ï¼ˆå¹¶å‘æ•°: {self.max_workers}ï¼‰...")
        if request_interval > 0:
            print(f"è¯·æ±‚é—´éš”: {request_interval} æ¯«ç§’ï¼ˆå¹¶è¡Œæ¨¡å¼ä¸‹ä»…ç”¨äºæ—¥å¿—ï¼‰")

        def update_progress(current):
            if progress_callback:
                progress = (current / total) * 100
                progress_callback(progress, total, current)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_id_info = {
                executor.submit(self._fetch_single_data, id_info): id_info
                for id_info in ids_list
            }

            # è·å–å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_id_info):
                id_info = future_to_id_info[future]

                try:
                    data_text, id_value, alias = future.result()

                    if data_text is not None:
                        # å°è¯•è§£ææ•°æ®ï¼Œè½¬æ¢ä¸ºä¸ DataFetcher ä¸€è‡´çš„æ ¼å¼
                        data_json = json.loads(data_text)
                        items = data_json.get("items", [])

                        if items:
                            results[id_value] = {}
                            id_to_name[id_value] = alias

                            for index, item in enumerate(items, 1):
                                title = item.get("title")
                                if (
                                    title is None
                                    or isinstance(title, float)
                                    or not str(title).strip()
                                ):
                                    continue
                                title = str(title).strip()
                                url = item.get("url", "")
                                mobile_url = item.get("mobileUrl", "")

                                if title in results[id_value]:
                                    results[id_value][title]["ranks"].append(index)
                                else:
                                    results[id_value][title] = {
                                        "ranks": [index],
                                        "url": url,
                                        "mobileUrl": mobile_url,
                                    }

                        update_progress(len([f for f in future_to_id_info if f.done()]))

                except Exception as e:
                    if isinstance(id_info, tuple):
                        id_value, _ = id_info
                    else:
                        id_value = id_info
                    failed_ids.append(id_value)
                    print(f"âŒ {id_value} è·å–å¤±è´¥: {e}")

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            completed = len([f for f in future_to_id_info if f.done()])
            update_progress(completed)

        successful_ids = [id_value for id_value, _ in future_to_id_info.items()]
        print(
            f"âœ… å¹¶è¡Œè·å–å®Œæˆï¼æˆåŠŸ: {len(successful_ids)}/{total}ï¼Œå¤±è´¥: {len(failed_ids)}"
        )

        return results, id_to_name, failed_ids


def parallel_fetch_all(
    ids_list: List[Union[str, Tuple[str, str]]],
    proxy_url: Optional[str] = None,
    api_url: Optional[str] = None,
    max_workers: int = 5,
    timeout: int = 15,
    progress_callback: Optional[callable] = None,
) -> Tuple[Dict, Dict, List]:
    """
    å¹¶è¡Œè·å–æ‰€æœ‰å¹³å°æ•°æ®çš„ä¾¿æ·å‡½æ•°

    Args:
        ids_list: å¹³å°IDåˆ—è¡¨
        proxy_url: ä»£ç†æœåŠ¡å™¨ URLï¼ˆå¯é€‰ï¼‰
        api_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤5ï¼‰
        timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤15ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        (ç»“æœå­—å…¸, IDåˆ°åç§°æ˜ å°„, å¤±è´¥IDåˆ—è¡¨)
    """
    fetcher = ParallelDataFetcher(
        proxy_url=proxy_url, api_url=api_url, max_workers=max_workers, timeout=timeout
    )

    return fetcher.fetch_all_parallel(ids_list, progress_callback)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_ids = [
        ("toutiao", "ä»Šæ—¥å¤´æ¡"),
        ("baidu", "ç™¾åº¦çƒ­æœ"),
        ("weibo", "å¾®åš"),
    ]

    def progress(progress, total, current):
        print(f"è¿›åº¦: {progress:.1f}% ({current}/{total})")

    results, id_to_name, failed_ids = parallel_fetch_all(
        test_ids, max_workers=3, timeout=10, progress_callback=progress
    )

    print(f"\nç»“æœ: {results}")
    print(f"IDæ˜ å°„: {id_to_name}")
    print(f"å¤±è´¥ID: {failed_ids}")
